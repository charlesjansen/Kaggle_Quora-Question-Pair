{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin\r\n",
      "command.sh\r\n",
      "floyd_requirements.txt\r\n",
      "glove.42B.300d.txt\r\n",
      "layer1rnnGPU.ipynb\r\n",
      "layer1rnnGPUbig.ipynb\r\n",
      "lstm_300_200_0.30_0.30.h5\r\n",
      "test_spacyLemma.csv\r\n",
      "test_spacyLemma_trigram.csv\r\n",
      "train_spacyLemma.csv\r\n",
      "train_spacyLemma_trigram.csv\r\n",
      "x_final_features_MyMagic3_spacyLemma_trigram.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#from spacy.en import English\n",
    "#nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://dl.dropboxusercontent.com/s/w5qxo2wy7vqbpra/test_spacyLemma_trigram.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://dl.dropboxusercontent.com/s/tw53gghntsnjzet/train_spacyLemma_trigram.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://dl.dropboxusercontent.com/s/py0sja1y2m1wtpp/x_final_features_MyMagic3_spacyLemma_trigram.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!gunzip x_final_features_MyMagic3_spacyLemma_trigram.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm glove.840B.300d.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!rm GoogleNews-vectors-negative300.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!rm glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm x_final_features_MyMagic3_spacyLemma_trigram.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11221M\r\n",
      "-rw-r--r-- 1 root root 3476M Dec 11  2013 GoogleNews-vectors-negative300.bin\r\n",
      "-rw-r--r-- 1 root root    1M May 25 15:07 command.sh\r\n",
      "-rw-r--r-- 1 root root    1M May 25 15:07 floyd_requirements.txt\r\n",
      "-rw-rw-r-- 1 root root 4793M Oct 24  2015 glove.42B.300d.txt\r\n",
      "-rw-r--r-- 1 root root    1M May 27 03:50 layer1rnnGPU.ipynb\r\n",
      "-rw-r--r-- 1 root root    1M May 27 03:57 layer1rnnGPUbig.ipynb\r\n",
      "-rw-r--r-- 1 root root  136M May 27 03:03 lstm_300_200_0.30_0.30.h5\r\n",
      "-rw-r--r-- 1 root root  306M May 25 15:18 test_spacyLemma.csv\r\n",
      "-rw-r--r-- 1 root root  294M May 27 00:54 test_spacyLemma_trigram.csv\r\n",
      "-rw-r--r-- 1 root root   59M May 25 15:18 train_spacyLemma.csv\r\n",
      "-rw-r--r-- 1 root root   56M May 27 00:54 train_spacyLemma_trigram.csv\r\n",
      "-rw-r--r-- 1 root root 2105M May 27 00:54 x_final_features_MyMagic3_spacyLemma_trigram.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -l --block-size=M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:       62879864     8799484    36735780      538060    17344600    53005284\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# New Lystdo LSTM    https://www.kaggle.com/lystdo/lb-0-18-lstm-with-glove-and-magic-features\n",
    "#==============================================================================\n",
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = ''\n",
    "EMBEDDING_FILE = BASE_DIR + 'glove.42B.300d.txt'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train_spacyLemma_trigram.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test_spacyLemma_trigram.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 300\n",
    "rate_drop_lstm = 0.3\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 1917493 word vectors of glove.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE, encoding=\"utf-8\")\n",
    "count = 0\n",
    "for line in f:\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "        continue\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 texts in train.csv\n",
      "Found 2345796 texts in test.csv\n",
      "Found 116784 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    if text == \"\":\n",
    "        text = \"-empty-\"\n",
    "    return(text)\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading x final features\n",
      "Index(['word_match', 'word_match_2root', 'shared_count', 'stops1_ratio',\n",
      "       'stops2_ratio', 'shared_2gram', 'cosine', 'words_hamming',\n",
      "       'diff_stops_r', 'len_q1', 'len_q2', 'diff_len', 'caps_count_q1',\n",
      "       'caps_count_q2', 'diff_caps', 'len_char_q1', 'len_char_q2',\n",
      "       'diff_len_char', 'len_word_q1', 'len_word_q2', 'diff_len_word',\n",
      "       'avg_world_len1', 'avg_world_len2', 'diff_avg_word', 'exactly_same',\n",
      "       'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what',\n",
      "       'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who',\n",
      "       'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when', 'q2_when',\n",
      "       'when_both', 'q1_why', 'q2_why', 'why_both', 'q1_freq', 'q2_freq',\n",
      "       'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
      "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
      "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd',\n",
      "       'cosine_distance', 'cityblock_distance', 'jaccard_distance',\n",
      "       'canberra_distance', 'euclidean_distance', 'minkowski_distance',\n",
      "       'braycurtis_distance', 'chebyshev_distance', 'correlation',\n",
      "       'sqeuclidean', 'levene', 'bartlett', 'ranksums', 'ansari', 'skew_q1vec',\n",
      "       'skew_q2vec', 'kur_q1vec', 'kur_q2vec', 'kur_Pearson_q1vec',\n",
      "       'kur_Pearson_q2vec', 'tmean_q1vec', 'tmean_q2vec', 'q1_q2_intersect',\n",
      "       'avgWordID1', 'avgWordID2', 'diffAvgWordID', 'diffRarestWordID',\n",
      "       'unigrams_common_count', 'unigrams_common_ratio', 'qid1_max_kcore',\n",
      "       'qid2_max_kcore'],\n",
      "      dtype='object')\n",
      "         word_match  word_match_2root  shared_count  stops1_ratio  \\\n",
      "count  2.749811e+06      2.749811e+06  2.750086e+06  2.750086e+06   \n",
      "mean   1.471691e-01      3.163215e-01  1.605620e+00  9.467299e-01   \n",
      "std    1.279563e-01      2.170479e-01  1.548127e+00  5.062331e-01   \n",
      "min    0.000000e+00      0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00      0.000000e+00  0.000000e+00  6.250000e-01   \n",
      "50%    1.338871e-01      3.659059e-01  1.000000e+00  8.333333e-01   \n",
      "75%    2.314808e-01      4.811245e-01  2.000000e+00  1.200000e+00   \n",
      "max    5.000000e-01      7.071068e-01  3.200000e+01  1.000000e+01   \n",
      "\n",
      "       stops2_ratio  shared_2gram        cosine  words_hamming  diff_stops_r  \\\n",
      "count  2.750086e+06  2.750086e+06  2.739236e+06   2.750086e+06  2.750086e+06   \n",
      "mean   9.507897e-01  7.278536e-02  2.967060e-01   1.303409e-01 -4.059835e-03   \n",
      "std    5.083392e-01  9.986519e-02  2.625285e-01   1.996097e-01  6.107030e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00 -8.000000e+00   \n",
      "25%    6.250000e-01  0.000000e+00  0.000000e+00   0.000000e+00 -3.214286e-01   \n",
      "50%    8.571429e-01  3.333333e-02  2.673381e-01   2.702703e-02  0.000000e+00   \n",
      "75%    1.200000e+00  1.111111e-01  4.727389e-01   1.818182e-01  3.000000e-01   \n",
      "max    9.000000e+00  5.000000e-01  1.000000e+00   1.000000e+00  8.400000e+00   \n",
      "\n",
      "             len_q1        len_q2      diff_len  caps_count_q1  caps_count_q2  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06   2.750086e+06   2.750086e+06   \n",
      "mean   6.003439e+01  6.003303e+01  1.365048e-03   2.750825e+00   2.769790e+00   \n",
      "std    3.111491e+01  3.222503e+01  3.462678e+01   2.563314e+00   2.579617e+00   \n",
      "min    1.000000e+00  1.000000e+00 -1.082000e+03   0.000000e+00   0.000000e+00   \n",
      "25%    4.000000e+01  3.900000e+01 -1.500000e+01   1.000000e+00   1.000000e+00   \n",
      "50%    5.300000e+01  5.200000e+01  0.000000e+00   2.000000e+00   2.000000e+00   \n",
      "75%    7.200000e+01  7.200000e+01  1.500000e+01   3.000000e+00   3.000000e+00   \n",
      "max    1.172000e+03  1.176000e+03  1.082000e+03   1.180000e+02   1.200000e+02   \n",
      "\n",
      "          diff_caps   len_char_q1   len_char_q2  diff_len_char   len_word_q1  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06   2.750086e+06  2.750086e+06   \n",
      "mean  -1.896486e-02  5.003641e+01  4.997047e+01   6.593321e-02  1.099250e+01   \n",
      "std    2.687006e+00  2.566831e+01  2.651310e+01   2.853981e+01  5.723078e+00   \n",
      "min   -1.140000e+02  1.000000e+00  1.000000e+00  -8.620000e+02  1.000000e+00   \n",
      "25%   -1.000000e+00  3.300000e+01  3.300000e+01  -1.200000e+01  7.000000e+00   \n",
      "50%    0.000000e+00  4.400000e+01  4.300000e+01   0.000000e+00  1.000000e+01   \n",
      "75%    1.000000e+00  6.000000e+01  6.000000e+01   1.300000e+01  1.300000e+01   \n",
      "max    1.120000e+02  9.450000e+02  9.480000e+02   8.620000e+02  2.380000e+02   \n",
      "\n",
      "        len_word_q2  diff_len_word  avg_world_len1  avg_world_len2  \\\n",
      "count  2.750086e+06   2.750086e+06    2.750086e+06    2.750086e+06   \n",
      "mean   1.105693e+01  -6.442999e-02    4.617793e+00    4.588066e+00   \n",
      "std    5.978650e+00   6.397039e+00    8.499471e-01    8.434182e-01   \n",
      "min    1.000000e+00  -2.230000e+02    1.000000e+00    1.000000e+00   \n",
      "25%    7.000000e+00  -3.000000e+00    4.000000e+00    4.000000e+00   \n",
      "50%    1.000000e+01   0.000000e+00    4.526316e+00    4.500000e+00   \n",
      "75%    1.300000e+01   3.000000e+00    5.095238e+00    5.000000e+00   \n",
      "max    2.380000e+02   2.200000e+02    5.100000e+01    3.600000e+01   \n",
      "\n",
      "       diff_avg_word  exactly_same    duplicated        q1_how        q2_how  \\\n",
      "count   2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean    2.972760e-02  2.181750e-05  1.963211e-03  2.653041e-01  2.671458e-01   \n",
      "std     9.495010e-01  4.670871e-03  4.426463e-02  4.414951e-01  4.424692e-01   \n",
      "min    -3.225000e+01  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    -5.000000e-01  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%     1.818182e-02  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%     5.555556e-01  0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00   \n",
      "max     4.688889e+01  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           how_both       q1_what       q2_what     what_both      q1_which  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   1.332475e-01  3.875544e-01  3.844949e-01  2.230301e-01  5.830799e-02   \n",
      "std    3.398421e-01  4.871920e-01  4.864757e-01  4.162784e-01  2.343250e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    0.000000e+00  1.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           q2_which    which_both        q1_who        q2_who      who_both  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   5.763783e-02  1.743109e-02  3.404221e-02  3.421457e-02  1.067967e-02   \n",
      "std    2.330574e-01  1.308711e-01  1.813377e-01  1.817799e-01  1.027892e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "           q1_where      q2_where    where_both       q1_when       q2_when  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   2.500795e-02  2.491122e-02  7.281590e-03  3.273570e-02  3.223208e-02   \n",
      "std    1.561491e-01  1.558546e-01  8.502101e-02  1.779441e-01  1.766159e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "          when_both        q1_why        q2_why      why_both       q1_freq  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   7.694668e-03  9.631372e-02  9.600136e-02  3.267607e-02  6.327904e+00   \n",
      "std    8.738114e-02  2.950210e-01  2.945932e-01  1.777874e-01  8.680526e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  2.698000e+03   \n",
      "\n",
      "            q2_freq  common_words   fuzz_qratio   fuzz_WRatio  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   6.397689e+00  4.563763e+00  5.391887e+01  7.112854e+01   \n",
      "std    8.684875e+01  2.568110e+00  1.652949e+01  1.701297e+01   \n",
      "min    1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.000000e+00  3.000000e+00  4.100000e+01  5.700000e+01   \n",
      "50%    1.000000e+00  4.000000e+00  5.100000e+01  7.600000e+01   \n",
      "75%    1.000000e+00  6.000000e+00  6.500000e+01  8.600000e+01   \n",
      "max    2.698000e+03  4.000000e+01  1.000000e+02  1.000000e+02   \n",
      "\n",
      "       fuzz_partial_ratio  fuzz_partial_token_set_ratio  \\\n",
      "count        2.750086e+06                  2.750086e+06   \n",
      "mean         5.910965e+01                  9.558811e+01   \n",
      "std          1.472779e+01                  1.533013e+01   \n",
      "min          0.000000e+00                  0.000000e+00   \n",
      "25%          4.700000e+01                  1.000000e+02   \n",
      "50%          5.700000e+01                  1.000000e+02   \n",
      "75%          6.900000e+01                  1.000000e+02   \n",
      "max          1.000000e+02                  1.000000e+02   \n",
      "\n",
      "       fuzz_partial_token_sort_ratio  fuzz_token_set_ratio  \\\n",
      "count                   2.750086e+06          2.750086e+06   \n",
      "mean                    6.016238e+01          6.358561e+01   \n",
      "std                     1.378302e+01          1.835790e+01   \n",
      "min                     0.000000e+00          0.000000e+00   \n",
      "25%                     5.000000e+01          5.000000e+01   \n",
      "50%                     5.900000e+01          6.300000e+01   \n",
      "75%                     6.900000e+01          7.700000e+01   \n",
      "max                     1.000000e+02          1.000000e+02   \n",
      "\n",
      "       fuzz_token_sort_ratio           wmd      norm_wmd  cosine_distance  \\\n",
      "count           2.750086e+06  2.750086e+06  2.750086e+06     2.618262e+06   \n",
      "mean            5.515594e+01           inf           inf     4.436199e-01   \n",
      "std             1.566141e+01           NaN           NaN     2.491417e-01   \n",
      "min             0.000000e+00  0.000000e+00  0.000000e+00    -1.998401e-15   \n",
      "25%             4.400000e+01  1.731025e+00  6.641877e-01     2.475121e-01   \n",
      "50%             5.400000e+01  2.441506e+00  9.221091e-01     4.119607e-01   \n",
      "75%             6.500000e+01  3.198627e+00  1.239759e+00     6.336241e-01   \n",
      "max             1.000000e+02           inf           inf     1.183050e+00   \n",
      "\n",
      "       cityblock_distance  jaccard_distance  canberra_distance  \\\n",
      "count        2.750086e+06      2.739963e+06       2.750086e+06   \n",
      "mean         1.237119e+01      9.763477e-01       1.664093e+02   \n",
      "std          4.131932e+00      1.519559e-01       4.905403e+01   \n",
      "min          0.000000e+00      0.000000e+00       0.000000e+00   \n",
      "25%          9.838357e+00      1.000000e+00       1.437888e+02   \n",
      "50%          1.278451e+01      1.000000e+00       1.680624e+02   \n",
      "75%          1.539840e+01      1.000000e+00       1.916406e+02   \n",
      "max          2.164893e+01      1.000000e+00       3.000000e+02   \n",
      "\n",
      "       euclidean_distance  minkowski_distance  braycurtis_distance  \\\n",
      "count        2.750086e+06        2.750086e+06         2.739963e+06   \n",
      "mean         8.943937e-01        4.035358e-01         5.447267e-01   \n",
      "std          2.983686e-01        1.347029e-01         2.312548e-01   \n",
      "min          0.000000e+00        0.000000e+00         0.000000e+00   \n",
      "25%          7.116895e-01        3.210453e-01         3.807838e-01   \n",
      "50%          9.245748e-01        4.171537e-01         5.205599e-01   \n",
      "75%          1.113414e+00        5.022930e-01         7.095505e-01   \n",
      "max          1.538214e+00        7.024272e-01         1.251399e+00   \n",
      "\n",
      "       chebyshev_distance   correlation   sqeuclidean        levene  \\\n",
      "count        2.750086e+06  2.618262e+06  2.750086e+06  2.739963e+06   \n",
      "mean         1.596173e-01  4.440933e-01  8.889639e-01  2.355619e+01   \n",
      "std          5.633671e-02  2.494175e-01  4.897368e-01  1.091655e+02   \n",
      "min          0.000000e+00 -2.220446e-15  0.000000e+00  0.000000e+00   \n",
      "25%          1.245938e-01  2.477380e-01  5.065019e-01  9.728374e-03   \n",
      "50%          1.620656e-01  4.124090e-01  8.548386e-01  4.937575e-02   \n",
      "75%          1.983721e-01  6.343039e-01  1.239690e+00  1.586010e-01   \n",
      "max          4.426990e-01  1.182977e+00  2.366101e+00  7.678342e+02   \n",
      "\n",
      "           bartlett      ranksums        ansari    skew_q1vec    skew_q2vec  \\\n",
      "count  2.739963e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean            inf -2.214964e-03  4.578892e+04  1.618348e-02  1.621178e-02   \n",
      "std             NaN  7.171994e-01  7.706254e+03  1.280562e-01  1.279051e-01   \n",
      "min   -4.539882e-13 -5.228235e+00  2.265000e+04 -6.762459e-01 -7.285284e-01   \n",
      "25%    4.232963e-05 -4.182588e-01  4.473700e+04 -6.662375e-02 -6.635021e-02   \n",
      "50%    4.860742e-04  0.000000e+00  4.515000e+04  1.583892e-02  1.609042e-02   \n",
      "75%    3.226283e-03  4.130777e-01  4.557500e+04  1.026939e-01  1.026081e-01   \n",
      "max             inf  5.369539e+00  9.015000e+04  7.380938e-01  7.615302e-01   \n",
      "\n",
      "          kur_q1vec     kur_q2vec  kur_Pearson_q1vec  kur_Pearson_q2vec  \\\n",
      "count  2.750086e+06  2.750086e+06       2.750086e+06       2.750086e+06   \n",
      "mean  -1.517093e-01 -1.505239e-01       2.848291e+00       2.849476e+00   \n",
      "std    5.311108e-01  5.256451e-01       5.311108e-01       5.256451e-01   \n",
      "min   -3.000000e+00 -3.000000e+00       0.000000e+00       0.000000e+00   \n",
      "25%   -2.670805e-01 -2.674345e-01       2.732919e+00       2.732566e+00   \n",
      "50%   -1.094844e-01 -1.101317e-01       2.890516e+00       2.889868e+00   \n",
      "75%    6.640914e-02  6.544592e-02       3.066409e+00       3.065446e+00   \n",
      "max    2.660471e+00  2.660409e+00       5.660471e+00       5.660409e+00   \n",
      "\n",
      "        tmean_q1vec   tmean_q2vec  q1_q2_intersect    avgWordID1  \\\n",
      "count  2.750086e+06  2.750086e+06     2.750086e+06  2.750086e+06   \n",
      "mean  -2.465169e-03 -2.453631e-03     4.940791e-01  3.820000e+03   \n",
      "std    2.877504e-03  2.868499e-03     4.555486e+00  6.268982e+03   \n",
      "min   -1.680878e-02 -1.568663e-02     0.000000e+00  0.000000e+00   \n",
      "25%   -4.383830e-03 -4.365613e-03     0.000000e+00  3.554444e+02   \n",
      "50%   -2.344513e-03 -2.330679e-03     0.000000e+00  1.141926e+03   \n",
      "75%   -3.461030e-04 -3.445430e-04     0.000000e+00  4.358949e+03   \n",
      "max    1.306164e-02  1.306164e-02     2.674000e+03  2.019243e+05   \n",
      "\n",
      "         avgWordID2  diffAvgWordID  diffRarestWordID  unigrams_common_count  \\\n",
      "count  2.750086e+06   2.750086e+06      2.750086e+06           2.750086e+06   \n",
      "mean   3.651728e+03   3.962563e+03      3.375488e+04           2.718327e+00   \n",
      "std    6.083242e+03   6.319902e+03      4.999440e+04           1.511392e+00   \n",
      "min    0.000000e+00   0.000000e+00      0.000000e+00           0.000000e+00   \n",
      "25%    3.429333e+02   2.299000e+02      7.590000e+02           2.000000e+00   \n",
      "50%    1.079857e+03   1.159000e+03      8.847000e+03           3.000000e+00   \n",
      "75%    4.049278e+03   5.048242e+03      4.650475e+04           4.000000e+00   \n",
      "max    2.022343e+05   1.896871e+05      2.558630e+05           2.700000e+01   \n",
      "\n",
      "       unigrams_common_ratio  qid1_max_kcore  qid2_max_kcore  \n",
      "count           2.750086e+06       2750086.0       2750086.0  \n",
      "mean            3.226344e-01             0.0             0.0  \n",
      "std             2.072569e-01             0.0             0.0  \n",
      "min             0.000000e+00             0.0             0.0  \n",
      "25%             1.600000e-01             0.0             0.0  \n",
      "50%             2.857143e-01             0.0             0.0  \n",
      "75%             4.285714e-01             0.0             0.0  \n",
      "max             1.000000e+00             0.0             0.0  \n",
      "Features: ['word_match', 'word_match_2root', 'shared_count', 'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine', 'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len', 'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1', 'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2', 'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word', 'exactly_same', 'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both', 'q1_freq', 'q2_freq', 'common_words', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cosine_distance', 'cityblock_distance', 'jaccard_distance', 'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance', 'chebyshev_distance', 'correlation', 'sqeuclidean', 'levene', 'bartlett', 'ranksums', 'ansari', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec', 'kur_Pearson_q1vec', 'kur_Pearson_q2vec', 'tmean_q1vec', 'tmean_q2vec', 'q1_q2_intersect', 'avgWordID1', 'avgWordID2', 'diffAvgWordID', 'diffRarestWordID', 'unigrams_common_count', 'unigrams_common_ratio', 'qid1_max_kcore', 'qid2_max_kcore']\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## generate leaky features\n",
    "########################################\n",
    "\n",
    "laptop = 0\n",
    "if laptop == 1:\n",
    "    drive = \"C\"\n",
    "else:\n",
    "    drive = \"F\"\n",
    "\n",
    "preprocessing = \"_MyMagic3_spacyLemma_trigram\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "from sklearn.model_selection import KFold\n",
    "K = 5\n",
    "kf = KFold(n_splits = K)\n",
    "#==============================================================================\n",
    "# print(list(enumerate(kf.split([1,2,3,4,5,6,7,8,9]))))   \n",
    "# X= np.array([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"])\n",
    "# for kth, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#     print(test_index)\n",
    "#     max = np.amax(test_index)+1\n",
    "#     min = np.amin(test_index)\n",
    "#     print(X[min:max])\n",
    "#==============================================================================\n",
    "\n",
    "np.set_printoptions(threshold=400000)\n",
    "pd.set_option('display.max_rows', 2000, 'display.max_columns', 2000,  'display.show_dimensions', 'truncate')\n",
    "\n",
    "\n",
    "RS = 12357\n",
    "np.random.seed(RS)\n",
    "\n",
    "print(\"Loading data\")\n",
    "input_folder = ''\n",
    "df_train = pd.read_csv(\"train_spacyLemma_trigram.csv\")\n",
    "df_test  = pd.read_csv(\"test_spacyLemma_trigram.csv\")\n",
    "df_test.drop([\"question1\", \"question2\"], axis=1, inplace=True)\n",
    "\n",
    "#adding final x features\n",
    "print(\"Loading x final features\")\n",
    "#x = pd.read_csv(drive + ':/DS-main/Kaggle-main/Quora Question Pairs - inputs/data/x_final_features.csv', header=0) \n",
    "#x = pd.read_csv(drive + ':/DS-main/Kaggle-main/Quora Question Pairs - inputs/data/x_final_features_expended.csv', header=0) \n",
    "#x = pd.read_csv(drive + ':/DS-main/Kaggle-main/Quora Question Pairs - inputs/data/x_final_features_lowRemoved.csv', header=0) \n",
    "x = pd.read_csv('x_final_features' + preprocessing  + '.csv', header=0) \n",
    "x = x.drop([\"rarestWordID1\", \"rarestWordID2\", 'max_kcore', \"tfidf_word_match\", \"wordMatchShare\"], axis=1)\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())\n",
    "\n",
    "feature_names = list(x.columns.values)\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "\n",
    "x_train = x[:df_train.shape[0]].astype(\"float64\")\n",
    "x_test_real  = x[df_train.shape[0]:].astype(\"float64\")\n",
    "labels = df_train['is_duplicate'].values\n",
    "x_train_real =  x_train\n",
    "labels_real =  labels\n",
    "del x, df_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# preprocessing Imputer\n",
    "####################################################################\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "x_train_real = Imputer().fit_transform(x_train_real)\n",
    "x_test_real = Imputer().fit_transform(x_test_real)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# preprocessing StandardScaler\n",
    "####################################################################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((x_train_real, x_test_real)))\n",
    "x_train_real = ss.transform(x_train_real)\n",
    "x_test_real = ss.transform(x_test_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 26079\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "#np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "x_train_real_train = np.vstack((x_train_real[idx_train], x_train_real[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "x_train_real_val = np.vstack((x_train_real[idx_val], x_train_real[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[116785,300]\n\t [[Node: embedding_1/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_1/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](embedding_1/embeddings, embedding_1/random_uniform)]]\n\nCaused by op 'embedding_1/embeddings/Assign', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-7cf58c825646>\", line 14, in <module>\n    embedded_sequences_1 = embedding_layer(sequence_1_input)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\", line 558, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python3.5/site-packages/keras/layers/embeddings.py\", line 101, in build\n    constraint=self.embeddings_constraint)\n  File \"/usr/local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\", line 391, in add_weight\n    weight = K.variable(initializer(shape), dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 317, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 306, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[116785,300]\n\t [[Node: embedding_1/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_1/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](embedding_1/embeddings, embedding_1/random_uniform)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[116785,300]\n\t [[Node: embedding_1/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_1/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](embedding_1/embeddings, embedding_1/random_uniform)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7cf58c825646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msequence_1_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0membedded_sequences_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_1_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_layer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_layer0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \"\"\"\n\u001b[1;32m   2102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[116785,300]\n\t [[Node: embedding_1/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_1/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](embedding_1/embeddings, embedding_1/random_uniform)]]\n\nCaused by op 'embedding_1/embeddings/Assign', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-7cf58c825646>\", line 14, in <module>\n    embedded_sequences_1 = embedding_layer(sequence_1_input)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\", line 558, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python3.5/site-packages/keras/layers/embeddings.py\", line 101, in build\n    constraint=self.embeddings_constraint)\n  File \"/usr/local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\", line 391, in add_weight\n    weight = K.variable(initializer(shape), dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 317, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 306, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[116785,300]\n\t [[Node: embedding_1/embeddings/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@embedding_1/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](embedding_1/embeddings, embedding_1/random_uniform)]]\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "#lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "lstm_layer0 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, return_sequences=True)\n",
    "lstm_layer1 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer0(embedded_sequences_1)\n",
    "x1 = lstm_layer0(x1)\n",
    "x1 = lstm_layer1(x1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer0(embedded_sequences_2)\n",
    "y1 = lstm_layer0(y1)\n",
    "y1 = lstm_layer1(y1)\n",
    "\n",
    "x_train_real_input = Input(shape=(x_train_real.shape[1],))\n",
    "x_train_real_dense = Dense(num_dense, activation=act)(x_train_real_input)\n",
    "x_train_real_dense = Dropout(rate_drop_dense)(x_train_real_dense)\n",
    "x_train_real_dense = BatchNormalization()(x_train_real_dense)\n",
    "x_train_real_dense = Dense(num_dense, activation=act)(x_train_real_dense)\n",
    "\n",
    "merged = concatenate([x1, y1, x_train_real_dense])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = concatenate([x1, y1, x_train_real_dense])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = concatenate([x1, y1, x_train_real_dense])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kernel active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_300_300_0.30_0.30\n",
      "Train on 727722 samples, validate on 80858 samples\n",
      "Epoch 1/2000\n",
      "Epoch 00000: val_loss improved from inf to 0.19061, saving model to lstm_300_300_0.30_0.30.h5\n",
      "981s - loss: 0.2276 - acc: 0.8321 - val_loss: 0.1906 - val_acc: 0.8580\n",
      "Epoch 2/2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, x_train_real_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=20)\n",
    "bst_model_path = STAMP + '_big.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "batch_size = 512\n",
    "hist = model.fit([data_1_train, data_2_train, x_train_real_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, x_train_real_val], labels_val, weight_val), \\\n",
    "        epochs=2000, batch_size=batch_size, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint], verbose=2)\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, x_test_real], batch_size=batch_size, verbose=2)\n",
    "preds += model.predict([test_data_2, test_data_1, x_test_real], batch_size=batch_size, verbose=2)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('newlystdo_%.4f_'%(bst_val_score)+STAMP+'_madeBig.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
