{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command.sh  floyd_requirements.txt  layer1rnnGPU.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/input': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.4.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 998kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.5/site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/79/8b/2a/b2da7fce57a1fd9b20b08fa8800c83b6fde62af9e880722e29\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-2.1.0.tar.gz (15.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 15.1MB 98kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.3 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.7.0 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Collecting smart_open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.3.tar.gz\n",
      "Collecting boto>=2.32 (from smart_open>=1.2.1->gensim)\n",
      "  Downloading boto-2.47.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bz2file (from smart_open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Collecting requests (from smart_open>=1.2.1->gensim)\n",
      "  Downloading requests-2.14.2-py2.py3-none-any.whl (560kB)\n",
      "\u001b[K    100% |████████████████████████████████| 563kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gensim, smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for gensim ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6f/f0/82/34d132e5a03ca765ec7f006d5f38ec7d749274a3f59c3fa5d0\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b0/81/ad/856aade935fceaab491a800ec4de58edb8642afa4c4ba91a00\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built gensim smart-open bz2file\n",
      "Installing collected packages: boto, bz2file, requests, smart-open, gensim\n",
      "Successfully installed boto-2.47.0 bz2file-0.98 gensim-2.1.0 requests-2.14.2 smart-open-1.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.0\n",
      "  Downloading tensorflow_gpu-1.0.0-cp35-cp35m-manylinux1_x86_64.whl (95.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 95.0MB 15kB/s  eta 0:00:01  7% |██▎                             | 6.7MB 45.8MB/s eta 0:00:02    11% |███▌                            | 10.5MB 36.5MB/s eta 0:00:03    61% |███████████████████▋            | 58.2MB 47.2MB/s eta 0:00:01�████████████        | 71.5MB 41.2MB/s eta 0:00:01    92% |█████████████████████████████▋  | 88.0MB 41.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.5/site-packages (from tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/site-packages (from tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/site-packages (from tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/site-packages (from tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/site-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.5/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0)\n",
      "Installing collected packages: tensorflow-gpu\n",
      "  Found existing installation: tensorflow-gpu 0.12.1\n",
      "    Uninstalling tensorflow-gpu-0.12.1:\n",
      "      Successfully uninstalled tensorflow-gpu-0.12.1\n",
      "Successfully installed tensorflow-gpu-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras==2.0.0 in /usr/local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/site-packages (from Keras==2.0.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.5/site-packages (from Keras==2.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/site-packages (from Keras==2.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/site-packages (from tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/site-packages (from tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.5/site-packages (from tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.5/site-packages (from tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/site-packages (from protobuf>=3.2.0->tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.5/site-packages (from setuptools->protobuf>=3.2.0->tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.5/site-packages (from setuptools->protobuf>=3.2.0->tensorflow->Keras==2.0.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.5/site-packages (from packaging>=16.8->setuptools->protobuf>=3.2.0->tensorflow->Keras==2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/site-packages (from h5py)\r\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.5/site-packages (from h5py)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-1.8.2.tar.gz (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 331kB/s eta 0:00:01    50% |████████████████                | 1.6MB 11.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.5/site-packages (from spacy)\n",
      "Collecting murmurhash<0.27,>=0.26 (from spacy)\n",
      "  Downloading murmurhash-0.26.4-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading cymem-1.31.2-cp35-cp35m-manylinux1_x86_64.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading preshed-1.0.0.tar.gz (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<6.6.0,>=6.5.0 (from spacy)\n",
      "  Downloading thinc-6.5.2.tar.gz (926kB)\n",
      "\u001b[K    100% |████████████████████████████████| 931kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/site-packages (from spacy)\n",
      "Collecting pathlib (from spacy)\n",
      "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ujson>=1.35 (from spacy)\n",
      "  Downloading ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3,>=0.2 (from spacy)\n",
      "  Downloading dill-0.2.6.zip (83kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.5/site-packages (from spacy)\n",
      "Collecting regex==2017.4.5 (from spacy)\n",
      "  Downloading regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K    100% |████████████████████████████████| 604kB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2 (from spacy)\n",
      "  Downloading ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading wrapt-1.10.10.tar.gz\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading tqdm-4.11.2-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cytoolz<0.9,>=0.8 (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading cytoolz-0.8.2.tar.gz (386kB)\n",
      "\u001b[K    100% |████████████████████████████████| 389kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.5/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Collecting toolz>=0.8.0 (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading toolz-0.8.2.tar.gz (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: spacy, preshed, thinc, pathlib, ujson, dill, regex, ftfy, wrapt, cytoolz, termcolor, toolz\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-^C\n",
      "\b \berror\n",
      "\u001b[31m  Failed building wheel for spacy\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for spacy\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#from spacy.en import English\n",
    "#nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-25 14:41:12--  https://dl.dropboxusercontent.com/s/pvyz8lewd7lyuum/GoogleNews-vectors-negative300.zip\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1282016977 (1.2G) [application/zip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.zip’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.19G  45.8MB/s    in 27s     \n",
      "\n",
      "2017-05-25 14:41:46 (46.1 MB/s) - ‘GoogleNews-vectors-negative300.zip’ saved [1282016977/1282016977]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/pvyz8lewd7lyuum/GoogleNews-vectors-negative300.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-25 14:41:46--  https://dl.dropboxusercontent.com/s/tnmz8pdkoqfawg6/test_spacyLemma.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 320217523 (305M) [text/csv]\n",
      "Saving to: ‘test_spacyLemma.csv’\n",
      "\n",
      "test_spacyLemma.csv 100%[===================>] 305.38M  56.4MB/s    in 6.0s    \n",
      "\n",
      "2017-05-25 14:41:54 (50.7 MB/s) - ‘test_spacyLemma.csv’ saved [320217523/320217523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/tnmz8pdkoqfawg6/test_spacyLemma.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-25 14:41:55--  https://dl.dropboxusercontent.com/s/tjln8kttfffb1gi/train_spacyLemma.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60869523 (58M) [text/csv]\n",
      "Saving to: ‘train_spacyLemma.csv’\n",
      "\n",
      "train_spacyLemma.cs 100%[===================>]  58.05M  31.5MB/s    in 1.8s    \n",
      "\n",
      "2017-05-25 14:41:57 (31.5 MB/s) - ‘train_spacyLemma.csv’ saved [60869523/60869523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/tjln8kttfffb1gi/train_spacyLemma.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!rm train_spacyLemma.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  GoogleNews-vectors-negative300.zip\n",
      " bunzipping: GoogleNews-vectors-negative300.bin  \n"
     ]
    }
   ],
   "source": [
    "!unzip GoogleNews-vectors-negative300.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1587M\r\n",
      "-rw-r--r-- 1 root root 1223M May 25 14:41 GoogleNews-vectors-negative300.zip\r\n",
      "-rw-r--r-- 1 root root    1M May 25 14:32 command.sh\r\n",
      "-rw-r--r-- 1 root root    1M May 25 14:32 floyd_requirements.txt\r\n",
      "-rw-r--r-- 1 root root    1M May 25 14:41 layer1rnnGPU.ipynb\r\n",
      "-rw-r--r-- 1 root root  306M May 25 14:41 test_spacyLemma.csv\r\n",
      "-rw-r--r-- 1 root root   59M May 25 14:41 train_spacyLemma.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -l --block-size=M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessing = \"_MyMagic3_spacyLemma\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "from sklearn.model_selection import KFold\n",
    "K = 5\n",
    "kf = KFold(n_splits = K)\n",
    "#==============================================================================\n",
    "# print(list(enumerate(kf.split([1,2,3,4,5,6,7,8,9]))))   \n",
    "# X= np.array([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"])\n",
    "# for kth, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#     print(test_index)\n",
    "#     max = np.amax(test_index)+1\n",
    "#     min = np.amin(test_index)\n",
    "#     print(X[min:max])\n",
    "#==============================================================================\n",
    "\n",
    "np.set_printoptions(threshold=400000)\n",
    "pd.set_option('display.max_rows', 2000, 'display.max_columns', 2000,  'display.show_dimensions', 'truncate')\n",
    "\n",
    "\n",
    "RS = 12357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GRU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "BASE_DIR = 'F:/DS-main/Kaggle-main/Quora Question Pairs - inputs/data/'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = 'train_spacyLemma.csv'\n",
    "TEST_DATA_FILE = 'test_spacyLemma.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 200\n",
    "rate_drop_lstm = 0.3\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "## index word vectors\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "proc train 100000\n",
      "proc train 200000\n",
      "proc train 300000\n",
      "proc train 400000\n",
      "Found 404290 texts in train.csv\n",
      "test 100000\n",
      "test 200000\n",
      "test 300000\n",
      "test 400000\n",
      "test 500000\n",
      "test 600000\n",
      "test 700000\n",
      "test 800000\n",
      "test 900000\n",
      "test 1000000\n",
      "test 1100000\n",
      "test 1200000\n",
      "test 1300000\n",
      "test 1400000\n",
      "test 1500000\n",
      "test 1600000\n",
      "test 1700000\n",
      "test 1800000\n",
      "test 1900000\n",
      "test 2000000\n",
      "test 2100000\n",
      "test 2200000\n",
      "test 2300000\n",
      "Found 2345796 texts in test.csv\n",
      "Found 116309 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "## process texts in datasets\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    if text == \"\":\n",
    "        text = \"empty\"\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(\"？\", \"?\", text) \n",
    "    text = re.sub(\"…\", \"\", text) \n",
    "    text = re.sub(\"é\", \"e\", text) \n",
    "    text = re.sub(r\" quikly \", \" quickly \", text)\n",
    "    text = re.sub(r\" unseccessful \", \" unsuccessful \", text)\n",
    "    text = re.sub(r\" addmision \", \" admission \", text)\n",
    "    text = re.sub(r\" insititute \", \" institute \", text)\n",
    "    text = re.sub(r\" connectionn \", \" connection \", text)\n",
    "    text = re.sub(r\" permantley \", \" permanently \", text)\n",
    "    text = re.sub(r\" sylabus \", \" syllabus \", text)\n",
    "    text = re.sub(r\" sequrity \", \" security \", text)\n",
    "    text = re.sub(r\" latop\", \" laptop\", text)\n",
    "    text = re.sub(r\" programmning \", \" programming \", text)  \n",
    "    text = re.sub(r\" begineer \", \" beginner \", text)  \n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" wtiter \", \" writer \", text)  \n",
    "    text = re.sub(r\" litrate \", \" literate \", text)  \n",
    "        \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "        if (len(texts_1)%100000 == 0 ):\n",
    "            print ('proc train',len(texts_1))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "        if (len(test_texts_1)%100000 == 0 ):\n",
    "            print ('test',len(test_texts_1))\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)\n",
    "\n",
    "del sequences_1, sequences_2, test_sequences_1, test_sequences_2, texts_1, texts_2, test_texts_1, test_texts_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 65681\n"
     ]
    }
   ],
   "source": [
    "## prepare embeddings\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************** FOLD  1  ******************\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3155611a7fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msequence_1_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0membedded_sequences_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_1_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m#x2 = lstm_layer1(x1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m                     \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0;31m# Perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0minput_shapes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0minput\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0moutput_shapes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0marguments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    759\u001b[0m                                         \u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                                         training=training) for _ in range(3)]\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_to_floatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(tensors, axis)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0marguments\u001b[0m \u001b[0muse\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mconvention\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m     \u001b[0mTheano\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0marange\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0monly\u001b[0m \u001b[0mone\u001b[0m \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m     \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfact\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m\"stop\"\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(concat_dim, values, name)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mA\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0moperation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mpopulated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcorresponding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    667\u001b[0m   \u001b[0mTensor\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalued\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mallows\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mops\u001b[0m \u001b[0mto\u001b[0m \u001b[0maccept\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m   \u001b[0mlists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscalars\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maddition\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m   \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mhas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mregistered\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mconversion\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          as_ref=False):\n\u001b[1;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 165\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 302\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "## sample train/validation data\n",
    "\n",
    "data_1_dbl = np.vstack((data_1, data_2))\n",
    "data_2_dbl = np.vstack((data_2, data_1))\n",
    "labels_dbl = np.concatenate((labels, labels))\n",
    "\n",
    "\n",
    "stacking_train = np.empty(shape=(len(data_1_dbl)))\n",
    "stacking_test = np.empty(shape=(len(test_data_1),K))\n",
    "for kth, (train_index, test_index) in enumerate(kf.split(data_1_dbl)):    \n",
    "    print(\"\\n********************** FOLD \", kth+1, \" ******************\\n\")\n",
    "    max_idx_test  = np.amax(test_index)+1\n",
    "    min_idx_test  = np.amin(test_index)\n",
    "    max_idx_train = np.amax(train_index)+1\n",
    "    min_idx_train = np.amin(train_index)\n",
    "    \n",
    "    \n",
    "    weight_val = np.ones(len(labels_dbl[min_idx_test:max_idx_test]))\n",
    "    if re_weight:\n",
    "        weight_val *= 0.472001959\n",
    "        weight_val[labels[min_idx_test:max_idx_test]==0] = 1.309028344\n",
    "        \n",
    "    ########################################\n",
    "    ## define the model structure\n",
    "    model = Sequential()\n",
    "    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "    #lstm_layer0 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, return_sequences=True)\n",
    "    gru_layer = GRU(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "    #lstm_layer1 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "    #lstm_layer = Bidirectional(GRU(num_lstm, return_sequences=True))\n",
    "    \n",
    "    \n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = gru_layer(embedded_sequences_1)\n",
    "    #x2 = lstm_layer1(x1)\n",
    "    \n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = gru_layer(embedded_sequences_2)\n",
    "    #y2 = lstm_layer1(y1)\n",
    "    \n",
    "    merged = concatenate([x1, y1])\n",
    "    #Imerged = concatenate([x2, y2])\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    \n",
    "    ########################################\n",
    "    ## add class weight\n",
    "    if re_weight:\n",
    "        class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    else:\n",
    "        class_weight = None\n",
    "    \n",
    "    ########################################\n",
    "    ## train the model\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer='nadam',\n",
    "            metrics=['acc'])\n",
    "    #model.summary()\n",
    "    print(STAMP)\n",
    "    \n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "    bst_model_path = STAMP + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    \n",
    "    hist = model.fit([data_1_dbl[min_idx_train:max_idx_train], data_2_dbl[min_idx_train:max_idx_train]], labels_dbl[min_idx_train:max_idx_train], \\\n",
    "        validation_data=([data_1_dbl[min_idx_test:max_idx_test], data_2_dbl[min_idx_test:max_idx_test]], labels_dbl[min_idx_test:max_idx_test], weight_val), \\\n",
    "        epochs=200, batch_size=512, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    model.load_weights(bst_model_path)\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "       #predict training ids of the testing fold\n",
    "    stacking_train[min_idx_test:max_idx_test] = model.predict([data_1_dbl[min_idx_test:max_idx_test], data_2_dbl[min_idx_test:max_idx_test]], batch_size=512, verbose=1)[:,0]\n",
    "    stacking_train[min_idx_test:max_idx_test] += model.predict([data_2_dbl[min_idx_test:max_idx_test], data_1_dbl[min_idx_test:max_idx_test]], batch_size=512, verbose=1)[:,0]\n",
    "    stacking_train[min_idx_test:max_idx_test] /= 2\n",
    "    #pred test\n",
    "    stacking_test[:,kth] = model.predict([test_data_1, test_data_2], batch_size=256, verbose=1)[:,0]\n",
    "    stacking_test[:,kth] += model.predict([test_data_2, test_data_1], batch_size=256, verbose=1)[:,0]\n",
    "    stacking_test[:,kth] /= 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Saving \n",
    "#pred test\n",
    "preds = (stacking_test[:,0] + stacking_test[:,1] + stacking_test[:,2] + stacking_test[:,3] + stacking_test[:,4])/5\n",
    "print(\"Writing output...\")\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = preds\n",
    "sub.to_csv(\"GRU_test_kf\" + preprocessing  + \".csv\", index=False)\n",
    "\n",
    "#--- pred training for ensemble\n",
    "print(\"Writing training pred output...\")\n",
    "sub = pd.DataFrame()\n",
    "sub['is_duplicate'] = stacking_train\n",
    "sub.to_csv(\"GRU_train_kf\" + preprocessing  + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
