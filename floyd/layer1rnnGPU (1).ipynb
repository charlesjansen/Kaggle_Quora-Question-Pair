{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command.sh  floyd_requirements.txt  layer1rnnGPU.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/input': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: scipy>=0.7.0 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.3 in /usr/local/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in /usr/local/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: Keras in /usr/local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: theano in /usr/local/lib/python3.5/site-packages (from Keras)\n",
      "Requirement already up-to-date: pyyaml in /usr/local/lib/python3.5/site-packages (from Keras)\n",
      "Requirement already up-to-date: six in /usr/local/lib/python3.5/site-packages (from Keras)\n",
      "Requirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python3.5/site-packages (from theano->Keras)\n",
      "Requirement already up-to-date: scipy>=0.14 in /usr/local/lib/python3.5/site-packages (from theano->Keras)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.5/site-packages (from h5py)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/site-packages (from h5py)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#from spacy.en import English\n",
    "#nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-25 15:17:19--  https://dl.dropboxusercontent.com/s/pvyz8lewd7lyuum/GoogleNews-vectors-negative300.zip\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1282016977 (1.2G) [application/zip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.zip’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.19G  58.5MB/s    in 24s     \n",
      "\n",
      "2017-05-25 15:17:52 (50.4 MB/s) - ‘GoogleNews-vectors-negative300.zip’ saved [1282016977/1282016977]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/pvyz8lewd7lyuum/GoogleNews-vectors-negative300.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-25 15:17:52--  https://dl.dropboxusercontent.com/s/tnmz8pdkoqfawg6/test_spacyLemma.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 320217523 (305M) [text/csv]\n",
      "Saving to: ‘test_spacyLemma.csv’\n",
      "\n",
      "test_spacyLemma.csv 100%[===================>] 305.38M  43.2MB/s    in 6.7s    \n",
      "\n",
      "2017-05-25 15:18:01 (45.4 MB/s) - ‘test_spacyLemma.csv’ saved [320217523/320217523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/tnmz8pdkoqfawg6/test_spacyLemma.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-25 15:18:01--  https://dl.dropboxusercontent.com/s/tjln8kttfffb1gi/train_spacyLemma.csv\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.1.6\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60869523 (58M) [text/csv]\n",
      "Saving to: ‘train_spacyLemma.csv’\n",
      "\n",
      "train_spacyLemma.cs 100%[===================>]  58.05M  67.5MB/s    in 0.9s    \n",
      "\n",
      "2017-05-25 15:18:07 (67.5 MB/s) - ‘train_spacyLemma.csv’ saved [60869523/60869523]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/tjln8kttfffb1gi/train_spacyLemma.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!rm train_spacyLemma.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  GoogleNews-vectors-negative300.zip\n",
      " bunzipping: GoogleNews-vectors-negative300.bin  \n"
     ]
    }
   ],
   "source": [
    "!unzip GoogleNews-vectors-negative300.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5062M\r\n",
      "-rw-r--r-- 1 root root 3476M Dec 11  2013 GoogleNews-vectors-negative300.bin\r\n",
      "-rw-r--r-- 1 root root 1223M May 25 15:17 GoogleNews-vectors-negative300.zip\r\n",
      "-rw-r--r-- 1 root root    1M May 25 15:07 command.sh\r\n",
      "-rw-r--r-- 1 root root    1M May 25 15:07 floyd_requirements.txt\r\n",
      "-rw-r--r-- 1 root root    1M May 25 15:20 layer1rnnGPU.ipynb\r\n",
      "-rw-r--r-- 1 root root  306M May 25 15:18 test_spacyLemma.csv\r\n",
      "-rw-r--r-- 1 root root   59M May 25 15:18 train_spacyLemma.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -l --block-size=M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = \"_MyMagic3_spacyLemma\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "from sklearn.model_selection import KFold\n",
    "K = 5\n",
    "kf = KFold(n_splits = K)\n",
    "#==============================================================================\n",
    "# print(list(enumerate(kf.split([1,2,3,4,5,6,7,8,9]))))   \n",
    "# X= np.array([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"])\n",
    "# for kth, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "#     print(test_index)\n",
    "#     max = np.amax(test_index)+1\n",
    "#     min = np.amin(test_index)\n",
    "#     print(X[min:max])\n",
    "#==============================================================================\n",
    "\n",
    "np.set_printoptions(threshold=400000)\n",
    "pd.set_option('display.max_rows', 2000, 'display.max_columns', 2000,  'display.show_dimensions', 'truncate')\n",
    "\n",
    "\n",
    "RS = 12357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GRU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "BASE_DIR = 'F:/DS-main/Kaggle-main/Quora Question Pairs - inputs/data/'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = 'train_spacyLemma.csv'\n",
    "TEST_DATA_FILE = 'test_spacyLemma.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 200\n",
    "rate_drop_lstm = 0.3\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "## index word vectors\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "proc train 100000\n",
      "proc train 200000\n",
      "proc train 300000\n",
      "proc train 400000\n",
      "Found 404290 texts in train.csv\n",
      "test 100000\n",
      "test 200000\n",
      "test 300000\n",
      "test 400000\n",
      "test 500000\n",
      "test 600000\n",
      "test 700000\n",
      "test 800000\n",
      "test 900000\n",
      "test 1000000\n",
      "test 1100000\n",
      "test 1200000\n",
      "test 1300000\n",
      "test 1400000\n",
      "test 1500000\n",
      "test 1600000\n",
      "test 1700000\n",
      "test 1800000\n",
      "test 1900000\n",
      "test 2000000\n",
      "test 2100000\n",
      "test 2200000\n",
      "test 2300000\n",
      "Found 2345796 texts in test.csv\n",
      "Found 116309 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "## process texts in datasets\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    if text == \"\":\n",
    "        text = \"empty\"\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(\"？\", \"?\", text) \n",
    "    text = re.sub(\"…\", \"\", text) \n",
    "    text = re.sub(\"é\", \"e\", text) \n",
    "    text = re.sub(r\" quikly \", \" quickly \", text)\n",
    "    text = re.sub(r\" unseccessful \", \" unsuccessful \", text)\n",
    "    text = re.sub(r\" addmision \", \" admission \", text)\n",
    "    text = re.sub(r\" insititute \", \" institute \", text)\n",
    "    text = re.sub(r\" connectionn \", \" connection \", text)\n",
    "    text = re.sub(r\" permantley \", \" permanently \", text)\n",
    "    text = re.sub(r\" sylabus \", \" syllabus \", text)\n",
    "    text = re.sub(r\" sequrity \", \" security \", text)\n",
    "    text = re.sub(r\" latop\", \" laptop\", text)\n",
    "    text = re.sub(r\" programmning \", \" programming \", text)  \n",
    "    text = re.sub(r\" begineer \", \" beginner \", text)  \n",
    "    text = re.sub(r\" qoura \", \" Quora \", text)\n",
    "    text = re.sub(r\" wtiter \", \" writer \", text)  \n",
    "    text = re.sub(r\" litrate \", \" literate \", text)  \n",
    "        \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "        if (len(texts_1)%100000 == 0 ):\n",
    "            print ('proc train',len(texts_1))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "        if (len(test_texts_1)%100000 == 0 ):\n",
    "            print ('test',len(test_texts_1))\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)\n",
    "\n",
    "del sequences_1, sequences_2, test_sequences_1, test_sequences_2, texts_1, texts_2, test_texts_1, test_texts_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 65681\n"
     ]
    }
   ],
   "source": [
    "## prepare embeddings\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************** FOLD  1  ******************\n",
      "\n",
      "lstm_300_200_0.30_0.30\n",
      "Train on 646864 samples, validate on 161716 samples\n",
      "Epoch 1/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.6880Epoch 00000: val_loss improved from inf to 0.34201, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.3912 - acc: 0.6880 - val_loss: 0.3420 - val_acc: 0.7485\n",
      "Epoch 2/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.7280Epoch 00001: val_loss improved from 0.34201 to 0.30572, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.3372 - acc: 0.7280 - val_loss: 0.3057 - val_acc: 0.7611\n",
      "Epoch 3/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.7473Epoch 00002: val_loss improved from 0.30572 to 0.29468, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.3176 - acc: 0.7474 - val_loss: 0.2947 - val_acc: 0.7765\n",
      "Epoch 4/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.7594Epoch 00003: val_loss improved from 0.29468 to 0.28477, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.3057 - acc: 0.7595 - val_loss: 0.2848 - val_acc: 0.8028\n",
      "Epoch 5/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.7680Epoch 00004: val_loss improved from 0.28477 to 0.27187, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2968 - acc: 0.7680 - val_loss: 0.2719 - val_acc: 0.8051\n",
      "Epoch 6/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.7750Epoch 00005: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2903 - acc: 0.7750 - val_loss: 0.2735 - val_acc: 0.8193\n",
      "Epoch 7/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.7807Epoch 00006: val_loss improved from 0.27187 to 0.25893, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2856 - acc: 0.7807 - val_loss: 0.2589 - val_acc: 0.8140\n",
      "Epoch 8/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.7841Epoch 00007: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2818 - acc: 0.7841 - val_loss: 0.2598 - val_acc: 0.8249\n",
      "Epoch 9/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.7876Epoch 00008: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2787 - acc: 0.7876 - val_loss: 0.2599 - val_acc: 0.8277\n",
      "Epoch 10/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.7905Epoch 00009: val_loss improved from 0.25893 to 0.25188, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2758 - acc: 0.7905 - val_loss: 0.2519 - val_acc: 0.8286\n",
      "Epoch 11/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.7924Epoch 00010: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2738 - acc: 0.7924 - val_loss: 0.2537 - val_acc: 0.8398\n",
      "Epoch 12/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.7949Epoch 00011: val_loss improved from 0.25188 to 0.24499, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2716 - acc: 0.7949 - val_loss: 0.2450 - val_acc: 0.8304\n",
      "Epoch 13/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.7962Epoch 00012: val_loss improved from 0.24499 to 0.24486, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2695 - acc: 0.7962 - val_loss: 0.2449 - val_acc: 0.8416\n",
      "Epoch 14/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.7986Epoch 00013: val_loss improved from 0.24486 to 0.24378, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2677 - acc: 0.7985 - val_loss: 0.2438 - val_acc: 0.8421\n",
      "Epoch 15/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.7995Epoch 00014: val_loss improved from 0.24378 to 0.24212, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2666 - acc: 0.7995 - val_loss: 0.2421 - val_acc: 0.8432\n",
      "Epoch 16/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.8006Epoch 00015: val_loss improved from 0.24212 to 0.23985, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2653 - acc: 0.8006 - val_loss: 0.2398 - val_acc: 0.8407\n",
      "Epoch 17/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.8023Epoch 00016: val_loss improved from 0.23985 to 0.23908, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2639 - acc: 0.8023 - val_loss: 0.2391 - val_acc: 0.8448\n",
      "Epoch 18/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.8040Epoch 00017: val_loss improved from 0.23908 to 0.23751, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2627 - acc: 0.8040 - val_loss: 0.2375 - val_acc: 0.8513\n",
      "Epoch 19/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.8045Epoch 00018: val_loss improved from 0.23751 to 0.23527, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2621 - acc: 0.8045 - val_loss: 0.2353 - val_acc: 0.8486\n",
      "Epoch 20/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.8052Epoch 00019: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2607 - acc: 0.8052 - val_loss: 0.2382 - val_acc: 0.8556\n",
      "Epoch 21/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.8064Epoch 00020: val_loss improved from 0.23527 to 0.22902, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 226s - loss: 0.2603 - acc: 0.8064 - val_loss: 0.2290 - val_acc: 0.8471\n",
      "Epoch 22/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.8072Epoch 00021: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2589 - acc: 0.8072 - val_loss: 0.2316 - val_acc: 0.8486\n",
      "Epoch 23/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.8082Epoch 00022: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2583 - acc: 0.8082 - val_loss: 0.2325 - val_acc: 0.8558\n",
      "Epoch 24/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.8090Epoch 00023: val_loss improved from 0.22902 to 0.22596, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2578 - acc: 0.8090 - val_loss: 0.2260 - val_acc: 0.8506\n",
      "Epoch 25/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.8098Epoch 00024: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2567 - acc: 0.8098 - val_loss: 0.2278 - val_acc: 0.8486\n",
      "Epoch 26/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.8106Epoch 00025: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2559 - acc: 0.8106 - val_loss: 0.2309 - val_acc: 0.8573\n",
      "Epoch 27/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.8111Epoch 00026: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2555 - acc: 0.8111 - val_loss: 0.2311 - val_acc: 0.8587\n",
      "Epoch 28/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.8109Epoch 00027: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646864/646864 [==============================] - 225s - loss: 0.2552 - acc: 0.8110 - val_loss: 0.2298 - val_acc: 0.8596\n",
      "Epoch 29/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.8112Epoch 00028: val_loss improved from 0.22596 to 0.22371, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2551 - acc: 0.8112 - val_loss: 0.2237 - val_acc: 0.8587\n",
      "Epoch 30/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.8120Epoch 00029: val_loss improved from 0.22371 to 0.22155, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2546 - acc: 0.8120 - val_loss: 0.2216 - val_acc: 0.8456\n",
      "Epoch 31/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.8131Epoch 00030: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2536 - acc: 0.8131 - val_loss: 0.2295 - val_acc: 0.8614\n",
      "Epoch 32/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8132Epoch 00031: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2531 - acc: 0.8132 - val_loss: 0.2271 - val_acc: 0.8619\n",
      "Epoch 33/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8133Epoch 00032: val_loss improved from 0.22155 to 0.22114, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2531 - acc: 0.8133 - val_loss: 0.2211 - val_acc: 0.8618\n",
      "Epoch 34/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.8137Epoch 00033: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2528 - acc: 0.8137 - val_loss: 0.2226 - val_acc: 0.8595\n",
      "Epoch 35/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.8141Epoch 00034: val_loss improved from 0.22114 to 0.21720, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2525 - acc: 0.8141 - val_loss: 0.2172 - val_acc: 0.8542\n",
      "Epoch 36/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.8151Epoch 00035: val_loss improved from 0.21720 to 0.21702, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2517 - acc: 0.8151 - val_loss: 0.2170 - val_acc: 0.8593\n",
      "Epoch 37/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.8158Epoch 00036: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2509 - acc: 0.8157 - val_loss: 0.2218 - val_acc: 0.8616\n",
      "Epoch 38/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.8160Epoch 00037: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2508 - acc: 0.8160 - val_loss: 0.2207 - val_acc: 0.8610\n",
      "Epoch 39/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.8163Epoch 00038: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2502 - acc: 0.8163 - val_loss: 0.2188 - val_acc: 0.8654\n",
      "Epoch 40/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.8164Epoch 00039: val_loss improved from 0.21702 to 0.21640, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2500 - acc: 0.8164 - val_loss: 0.2164 - val_acc: 0.8579\n",
      "Epoch 41/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.8164Epoch 00040: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2498 - acc: 0.8164 - val_loss: 0.2226 - val_acc: 0.8649\n",
      "Epoch 42/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.8170Epoch 00041: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2497 - acc: 0.8170 - val_loss: 0.2196 - val_acc: 0.8588\n",
      "Epoch 43/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.8175Epoch 00042: val_loss improved from 0.21640 to 0.21552, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2493 - acc: 0.8175 - val_loss: 0.2155 - val_acc: 0.8606\n",
      "Epoch 44/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.8175Epoch 00043: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2488 - acc: 0.8175 - val_loss: 0.2176 - val_acc: 0.8646\n",
      "Epoch 45/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.8185Epoch 00044: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2480 - acc: 0.8185 - val_loss: 0.2157 - val_acc: 0.8616\n",
      "Epoch 46/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.8182Epoch 00045: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2483 - acc: 0.8182 - val_loss: 0.2157 - val_acc: 0.8668\n",
      "Epoch 47/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.8186Epoch 00046: val_loss improved from 0.21552 to 0.21334, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2479 - acc: 0.8186 - val_loss: 0.2133 - val_acc: 0.8636\n",
      "Epoch 48/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.8186Epoch 00047: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2477 - acc: 0.8186 - val_loss: 0.2155 - val_acc: 0.8636\n",
      "Epoch 49/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.8194Epoch 00048: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2470 - acc: 0.8194 - val_loss: 0.2162 - val_acc: 0.8656\n",
      "Epoch 50/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8195Epoch 00049: val_loss improved from 0.21334 to 0.21292, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2466 - acc: 0.8195 - val_loss: 0.2129 - val_acc: 0.8587\n",
      "Epoch 51/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8203Epoch 00050: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2466 - acc: 0.8203 - val_loss: 0.2163 - val_acc: 0.8660\n",
      "Epoch 52/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.8203Epoch 00051: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2459 - acc: 0.8203 - val_loss: 0.2168 - val_acc: 0.8693\n",
      "Epoch 53/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.8211Epoch 00052: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2458 - acc: 0.8211 - val_loss: 0.2136 - val_acc: 0.8662\n",
      "Epoch 54/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.8209Epoch 00053: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2457 - acc: 0.8209 - val_loss: 0.2143 - val_acc: 0.8687\n",
      "Epoch 55/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.8216Epoch 00054: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2451 - acc: 0.8216 - val_loss: 0.2173 - val_acc: 0.8672\n",
      "Epoch 56/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.8212Epoch 00055: val_loss improved from 0.21292 to 0.21213, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2453 - acc: 0.8212 - val_loss: 0.2121 - val_acc: 0.8654\n",
      "Epoch 57/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.8215Epoch 00056: val_loss improved from 0.21213 to 0.20967, saving model to lstm_300_200_0.30_0.30.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646864/646864 [==============================] - 227s - loss: 0.2450 - acc: 0.8215 - val_loss: 0.2097 - val_acc: 0.8623\n",
      "Epoch 58/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.8212Epoch 00057: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2451 - acc: 0.8212 - val_loss: 0.2163 - val_acc: 0.8704\n",
      "Epoch 59/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.8216Epoch 00058: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2447 - acc: 0.8216 - val_loss: 0.2198 - val_acc: 0.8735\n",
      "Epoch 60/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.8223Epoch 00059: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2445 - acc: 0.8223 - val_loss: 0.2121 - val_acc: 0.8679\n",
      "Epoch 61/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.8229Epoch 00060: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2438 - acc: 0.8229 - val_loss: 0.2103 - val_acc: 0.8720\n",
      "Epoch 62/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.8227Epoch 00061: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2438 - acc: 0.8227 - val_loss: 0.2122 - val_acc: 0.8731\n",
      "Epoch 63/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.8224Epoch 00062: val_loss did not improve\n",
      "646864/646864 [==============================] - 225s - loss: 0.2438 - acc: 0.8224 - val_loss: 0.2167 - val_acc: 0.8730\n",
      "161716/161716 [==============================] - 19s    \n",
      "161716/161716 [==============================] - 19s    \n",
      "2345472/2345796 [============================>.] - ETA: 0s\n",
      "********************** FOLD  2  ******************\n",
      "\n",
      "lstm_300_200_0.30_0.30\n",
      "Train on 808580 samples, validate on 161716 samples\n",
      "Epoch 1/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3834 - acc: 0.6933Epoch 00000: val_loss improved from inf to 0.33081, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.3834 - acc: 0.6933 - val_loss: 0.3308 - val_acc: 0.7634\n",
      "Epoch 2/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.7350Epoch 00001: val_loss improved from 0.33081 to 0.30042, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.3297 - acc: 0.7350 - val_loss: 0.3004 - val_acc: 0.7862\n",
      "Epoch 3/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.7533Epoch 00002: val_loss improved from 0.30042 to 0.27629, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.3110 - acc: 0.7533 - val_loss: 0.2763 - val_acc: 0.7937\n",
      "Epoch 4/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.7658Epoch 00003: val_loss improved from 0.27629 to 0.26941, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2992 - acc: 0.7658 - val_loss: 0.2694 - val_acc: 0.8215\n",
      "Epoch 5/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.7741Epoch 00004: val_loss improved from 0.26941 to 0.25554, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2911 - acc: 0.7741 - val_loss: 0.2555 - val_acc: 0.8200\n",
      "Epoch 6/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.7797Epoch 00005: val_loss improved from 0.25554 to 0.25478, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2852 - acc: 0.7797 - val_loss: 0.2548 - val_acc: 0.8353\n",
      "Epoch 7/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.7841Epoch 00006: val_loss improved from 0.25478 to 0.23905, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2812 - acc: 0.7841 - val_loss: 0.2391 - val_acc: 0.8379\n",
      "Epoch 8/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.7880Epoch 00007: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2772 - acc: 0.7879 - val_loss: 0.2423 - val_acc: 0.8500\n",
      "Epoch 9/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.7915Epoch 00008: val_loss improved from 0.23905 to 0.23073, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2742 - acc: 0.7915 - val_loss: 0.2307 - val_acc: 0.8418\n",
      "Epoch 10/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.7933Epoch 00009: val_loss improved from 0.23073 to 0.23073, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2722 - acc: 0.7933 - val_loss: 0.2307 - val_acc: 0.8469\n",
      "Epoch 11/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.7957Epoch 00010: val_loss improved from 0.23073 to 0.22558, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2698 - acc: 0.7957 - val_loss: 0.2256 - val_acc: 0.8523\n",
      "Epoch 12/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.7972Epoch 00011: val_loss improved from 0.22558 to 0.22221, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2684 - acc: 0.7972 - val_loss: 0.2222 - val_acc: 0.8544\n",
      "Epoch 13/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.7991Epoch 00012: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2667 - acc: 0.7991 - val_loss: 0.2239 - val_acc: 0.8630\n",
      "Epoch 14/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.8007Epoch 00013: val_loss improved from 0.22221 to 0.21444, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2650 - acc: 0.8007 - val_loss: 0.2144 - val_acc: 0.8541\n",
      "Epoch 15/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.8026Epoch 00014: val_loss improved from 0.21444 to 0.21023, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2629 - acc: 0.8026 - val_loss: 0.2102 - val_acc: 0.8598\n",
      "Epoch 16/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.8033Epoch 00015: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2626 - acc: 0.8033 - val_loss: 0.2145 - val_acc: 0.8608\n",
      "Epoch 17/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.8040Epoch 00016: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2617 - acc: 0.8040 - val_loss: 0.2121 - val_acc: 0.8578\n",
      "Epoch 18/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.8053Epoch 00017: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2607 - acc: 0.8053 - val_loss: 0.2136 - val_acc: 0.8715\n",
      "Epoch 19/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.8063Epoch 00018: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2599 - acc: 0.8063 - val_loss: 0.2146 - val_acc: 0.8747\n",
      "Epoch 20/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.8072Epoch 00019: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2584 - acc: 0.8072 - val_loss: 0.2157 - val_acc: 0.8801\n",
      "Epoch 21/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.8085Epoch 00020: val_loss improved from 0.21023 to 0.20485, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2578 - acc: 0.8085 - val_loss: 0.2049 - val_acc: 0.8675\n",
      "Epoch 22/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.8088Epoch 00021: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2571 - acc: 0.8088 - val_loss: 0.2091 - val_acc: 0.8767\n",
      "Epoch 23/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.8092Epoch 00022: val_loss improved from 0.20485 to 0.19961, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2569 - acc: 0.8092 - val_loss: 0.1996 - val_acc: 0.8713\n",
      "Epoch 24/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.8106Epoch 00023: val_loss improved from 0.19961 to 0.19867, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2557 - acc: 0.8106 - val_loss: 0.1987 - val_acc: 0.8755\n",
      "Epoch 25/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.8113Epoch 00024: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2547 - acc: 0.8113 - val_loss: 0.2040 - val_acc: 0.8824\n",
      "Epoch 26/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.8117Epoch 00025: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2547 - acc: 0.8117 - val_loss: 0.1995 - val_acc: 0.8758\n",
      "Epoch 27/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.8119Epoch 00026: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2540 - acc: 0.8119 - val_loss: 0.1998 - val_acc: 0.8739\n",
      "Epoch 28/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.8122Epoch 00027: val_loss improved from 0.19867 to 0.19650, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2537 - acc: 0.8122 - val_loss: 0.1965 - val_acc: 0.8774\n",
      "Epoch 29/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.8134Epoch 00028: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2527 - acc: 0.8134 - val_loss: 0.2006 - val_acc: 0.8788\n",
      "Epoch 30/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.8135Epoch 00029: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2525 - acc: 0.8135 - val_loss: 0.2014 - val_acc: 0.8785\n",
      "Epoch 31/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.8134Epoch 00030: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2525 - acc: 0.8134 - val_loss: 0.1970 - val_acc: 0.8784\n",
      "Epoch 32/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.8151Epoch 00031: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2514 - acc: 0.8151 - val_loss: 0.1966 - val_acc: 0.8823\n",
      "Epoch 33/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.8147Epoch 00032: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2513 - acc: 0.8147 - val_loss: 0.2030 - val_acc: 0.8824\n",
      "Epoch 34/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.8152Epoch 00033: val_loss improved from 0.19650 to 0.19416, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2508 - acc: 0.8152 - val_loss: 0.1942 - val_acc: 0.8819\n",
      "Epoch 35/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.8161Epoch 00034: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2497 - acc: 0.8161 - val_loss: 0.1956 - val_acc: 0.8854\n",
      "Epoch 36/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.8162Epoch 00035: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2502 - acc: 0.8162 - val_loss: 0.1990 - val_acc: 0.8881\n",
      "Epoch 37/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.8159Epoch 00036: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2497 - acc: 0.8159 - val_loss: 0.1985 - val_acc: 0.8891\n",
      "Epoch 38/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.8167Epoch 00037: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2491 - acc: 0.8167 - val_loss: 0.2001 - val_acc: 0.8908\n",
      "Epoch 39/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.8174Epoch 00038: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2486 - acc: 0.8174 - val_loss: 0.2008 - val_acc: 0.8916\n",
      "Epoch 40/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.8176Epoch 00039: val_loss improved from 0.19416 to 0.19078, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2486 - acc: 0.8176 - val_loss: 0.1908 - val_acc: 0.8818\n",
      "Epoch 41/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.8178Epoch 00040: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2484 - acc: 0.8178 - val_loss: 0.1932 - val_acc: 0.8922\n",
      "Epoch 42/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.8185Epoch 00041: val_loss improved from 0.19078 to 0.18951, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2474 - acc: 0.8185 - val_loss: 0.1895 - val_acc: 0.8865\n",
      "Epoch 43/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.8186Epoch 00042: val_loss improved from 0.18951 to 0.18945, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2481 - acc: 0.8186 - val_loss: 0.1894 - val_acc: 0.8852\n",
      "Epoch 44/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.8185Epoch 00043: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2472 - acc: 0.8185 - val_loss: 0.1908 - val_acc: 0.8883\n",
      "Epoch 45/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.8183Epoch 00044: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2481 - acc: 0.8183 - val_loss: 0.1979 - val_acc: 0.8929\n",
      "Epoch 46/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.8175Epoch 00045: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2491 - acc: 0.8175 - val_loss: 0.1930 - val_acc: 0.8822\n",
      "Epoch 47/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.8178Epoch 00046: val_loss improved from 0.18945 to 0.18568, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2485 - acc: 0.8179 - val_loss: 0.1857 - val_acc: 0.8816\n",
      "Epoch 48/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.8186Epoch 00047: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2470 - acc: 0.8185 - val_loss: 0.1921 - val_acc: 0.8920\n",
      "Epoch 49/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.8195Epoch 00048: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2467 - acc: 0.8195 - val_loss: 0.1879 - val_acc: 0.8907\n",
      "Epoch 50/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.8201Epoch 00049: val_loss improved from 0.18568 to 0.18518, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2461 - acc: 0.8201 - val_loss: 0.1852 - val_acc: 0.8887\n",
      "Epoch 51/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.8196Epoch 00050: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808580/808580 [==============================] - 277s - loss: 0.2463 - acc: 0.8196 - val_loss: 0.1880 - val_acc: 0.8896\n",
      "Epoch 52/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.8200Epoch 00051: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2461 - acc: 0.8200 - val_loss: 0.1868 - val_acc: 0.8902\n",
      "Epoch 53/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.8195Epoch 00052: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2467 - acc: 0.8195 - val_loss: 0.1869 - val_acc: 0.8835\n",
      "Epoch 54/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.8201Epoch 00053: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2460 - acc: 0.8201 - val_loss: 0.1886 - val_acc: 0.8912\n",
      "Epoch 55/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.8180Epoch 00054: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2485 - acc: 0.8180 - val_loss: 0.1866 - val_acc: 0.8863\n",
      "Epoch 56/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.8204Epoch 00055: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2462 - acc: 0.8204 - val_loss: 0.1953 - val_acc: 0.8977\n",
      "161716/161716 [==============================] - 19s    \n",
      "161716/161716 [==============================] - 19s    \n",
      "2345472/2345796 [============================>.] - ETA: 0s\n",
      "********************** FOLD  3  ******************\n",
      "\n",
      "lstm_300_200_0.30_0.30\n",
      "Train on 808580 samples, validate on 161716 samples\n",
      "Epoch 1/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.6926Epoch 00000: val_loss improved from inf to 0.27791, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 280s - loss: 0.3844 - acc: 0.6926 - val_loss: 0.2779 - val_acc: 0.7548\n",
      "Epoch 2/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.7356Epoch 00001: val_loss improved from 0.27791 to 0.24939, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.3303 - acc: 0.7356 - val_loss: 0.2494 - val_acc: 0.7749\n",
      "Epoch 3/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.7545Epoch 00002: val_loss improved from 0.24939 to 0.23487, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.3112 - acc: 0.7545 - val_loss: 0.2349 - val_acc: 0.8083\n",
      "Epoch 4/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.7661Epoch 00003: val_loss improved from 0.23487 to 0.22073, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2993 - acc: 0.7661 - val_loss: 0.2207 - val_acc: 0.8268\n",
      "Epoch 5/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.7745Epoch 00004: val_loss improved from 0.22073 to 0.21040, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2913 - acc: 0.7745 - val_loss: 0.2104 - val_acc: 0.8244\n",
      "Epoch 6/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.7807Epoch 00005: val_loss improved from 0.21040 to 0.20428, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2852 - acc: 0.7807 - val_loss: 0.2043 - val_acc: 0.8249\n",
      "Epoch 7/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.7851Epoch 00006: val_loss improved from 0.20428 to 0.19908, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2810 - acc: 0.7851 - val_loss: 0.1991 - val_acc: 0.8372\n",
      "Epoch 8/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.7888Epoch 00007: val_loss improved from 0.19908 to 0.19633, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2775 - acc: 0.7888 - val_loss: 0.1963 - val_acc: 0.8384\n",
      "Epoch 9/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.7917Epoch 00008: val_loss improved from 0.19633 to 0.18985, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2742 - acc: 0.7917 - val_loss: 0.1899 - val_acc: 0.8513\n",
      "Epoch 10/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.7948Epoch 00009: val_loss improved from 0.18985 to 0.18675, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.2717 - acc: 0.7948 - val_loss: 0.1868 - val_acc: 0.8586\n",
      "Epoch 11/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.7707Epoch 00010: val_loss did not improve\n",
      "808580/808580 [==============================] - 277s - loss: 0.2984 - acc: 0.7707 - val_loss: 0.2153 - val_acc: 0.8254\n",
      "Epoch 12/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.7744Epoch 00011: val_loss did not improve\n",
      "808580/808580 [==============================] - 278s - loss: 0.2938 - acc: 0.7744 - val_loss: 0.2013 - val_acc: 0.8402\n",
      "Epoch 13/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.7759Epoch 00012: val_loss did not improve\n",
      "808580/808580 [==============================] - 278s - loss: 0.2927 - acc: 0.7759 - val_loss: 0.2084 - val_acc: 0.8245\n",
      "Epoch 14/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.7813Epoch 00013: val_loss did not improve\n",
      "808580/808580 [==============================] - 278s - loss: 0.2862 - acc: 0.7814 - val_loss: 0.1975 - val_acc: 0.8506\n",
      "Epoch 15/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.7884Epoch 00014: val_loss did not improve\n",
      "808580/808580 [==============================] - 278s - loss: 0.2788 - acc: 0.7884 - val_loss: 0.1909 - val_acc: 0.8520\n",
      "Epoch 16/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.7921Epoch 00015: val_loss did not improve\n",
      "808580/808580 [==============================] - 278s - loss: 0.2748 - acc: 0.7920 - val_loss: 0.1896 - val_acc: 0.8593\n",
      "161716/161716 [==============================] - 20s    \n",
      "161716/161716 [==============================] - 19s    \n",
      "2345472/2345796 [============================>.] - ETA: 0s\n",
      "********************** FOLD  4  ******************\n",
      "\n",
      "lstm_300_200_0.30_0.30\n",
      "Train on 808580 samples, validate on 161716 samples\n",
      "Epoch 1/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.6941Epoch 00000: val_loss improved from inf to 0.24643, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 279s - loss: 0.3826 - acc: 0.6941 - val_loss: 0.2464 - val_acc: 0.7303\n",
      "Epoch 2/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.7362Epoch 00001: val_loss improved from 0.24643 to 0.20649, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 277s - loss: 0.3299 - acc: 0.7362 - val_loss: 0.2065 - val_acc: 0.7736\n",
      "Epoch 3/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.7547Epoch 00002: val_loss improved from 0.20649 to 0.19067, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 277s - loss: 0.3112 - acc: 0.7547 - val_loss: 0.1907 - val_acc: 0.7896\n",
      "Epoch 4/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.7649Epoch 00003: val_loss improved from 0.19067 to 0.17338, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 277s - loss: 0.3004 - acc: 0.7649 - val_loss: 0.1734 - val_acc: 0.8185\n",
      "Epoch 5/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.7740Epoch 00004: val_loss improved from 0.17338 to 0.16520, saving model to lstm_300_200_0.30_0.30.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808580/808580 [==============================] - 278s - loss: 0.2924 - acc: 0.7740 - val_loss: 0.1652 - val_acc: 0.8307\n",
      "Epoch 6/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.7798Epoch 00005: val_loss improved from 0.16520 to 0.16033, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2864 - acc: 0.7798 - val_loss: 0.1603 - val_acc: 0.8362\n",
      "Epoch 7/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.7840Epoch 00006: val_loss improved from 0.16033 to 0.16015, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2823 - acc: 0.7840 - val_loss: 0.1602 - val_acc: 0.8329\n",
      "Epoch 8/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.7880Epoch 00007: val_loss improved from 0.16015 to 0.15351, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2785 - acc: 0.7880 - val_loss: 0.1535 - val_acc: 0.8435\n",
      "Epoch 9/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.7908Epoch 00008: val_loss improved from 0.15351 to 0.15129, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2755 - acc: 0.7908 - val_loss: 0.1513 - val_acc: 0.8454\n",
      "Epoch 10/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.7935Epoch 00009: val_loss improved from 0.15129 to 0.14827, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2730 - acc: 0.7935 - val_loss: 0.1483 - val_acc: 0.8525\n",
      "Epoch 11/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.7946Epoch 00010: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2718 - acc: 0.7946 - val_loss: 0.1559 - val_acc: 0.8402\n",
      "Epoch 12/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.7960Epoch 00011: val_loss improved from 0.14827 to 0.14401, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2710 - acc: 0.7960 - val_loss: 0.1440 - val_acc: 0.8577\n",
      "Epoch 13/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.7980Epoch 00012: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2685 - acc: 0.7980 - val_loss: 0.1445 - val_acc: 0.8548\n",
      "Epoch 14/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.8001Epoch 00013: val_loss improved from 0.14401 to 0.14363, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2669 - acc: 0.8001 - val_loss: 0.1436 - val_acc: 0.8564\n",
      "Epoch 15/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.8005Epoch 00014: val_loss improved from 0.14363 to 0.14295, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 277s - loss: 0.2660 - acc: 0.8005 - val_loss: 0.1429 - val_acc: 0.8555\n",
      "Epoch 16/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.8021Epoch 00015: val_loss improved from 0.14295 to 0.14278, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2645 - acc: 0.8021 - val_loss: 0.1428 - val_acc: 0.8545\n",
      "Epoch 17/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.8030Epoch 00016: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2635 - acc: 0.8030 - val_loss: 0.1448 - val_acc: 0.8518\n",
      "Epoch 18/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.8038Epoch 00017: val_loss improved from 0.14278 to 0.14096, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2625 - acc: 0.8038 - val_loss: 0.1410 - val_acc: 0.8577\n",
      "Epoch 19/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.8057Epoch 00018: val_loss improved from 0.14096 to 0.13656, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2612 - acc: 0.8056 - val_loss: 0.1366 - val_acc: 0.8653\n",
      "Epoch 20/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.8063Epoch 00019: val_loss improved from 0.13656 to 0.12985, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2606 - acc: 0.8063 - val_loss: 0.1299 - val_acc: 0.8782\n",
      "Epoch 21/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.8064Epoch 00020: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2599 - acc: 0.8064 - val_loss: 0.1317 - val_acc: 0.8729\n",
      "Epoch 22/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.8073Epoch 00021: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2592 - acc: 0.8073 - val_loss: 0.1396 - val_acc: 0.8592\n",
      "Epoch 23/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.8086Epoch 00022: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2580 - acc: 0.8086 - val_loss: 0.1316 - val_acc: 0.8735\n",
      "Epoch 24/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.8086Epoch 00023: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2577 - acc: 0.8086 - val_loss: 0.1300 - val_acc: 0.8763\n",
      "Epoch 25/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.8100Epoch 00024: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2566 - acc: 0.8100 - val_loss: 0.1313 - val_acc: 0.8727\n",
      "Epoch 26/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.8110Epoch 00025: val_loss improved from 0.12985 to 0.12957, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2560 - acc: 0.8110 - val_loss: 0.1296 - val_acc: 0.8759\n",
      "Epoch 27/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.8107Epoch 00026: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2558 - acc: 0.8107 - val_loss: 0.1322 - val_acc: 0.8713\n",
      "Epoch 28/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.8110Epoch 00027: val_loss improved from 0.12957 to 0.12905, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2558 - acc: 0.8110 - val_loss: 0.1291 - val_acc: 0.8761\n",
      "Epoch 29/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.8119Epoch 00028: val_loss improved from 0.12905 to 0.12820, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2550 - acc: 0.8120 - val_loss: 0.1282 - val_acc: 0.8775\n",
      "Epoch 30/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.8040Epoch 00029: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2638 - acc: 0.8040 - val_loss: 0.1290 - val_acc: 0.8769\n",
      "Epoch 31/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.8093Epoch 00030: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2577 - acc: 0.8093 - val_loss: 0.1291 - val_acc: 0.8757\n",
      "Epoch 32/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.8118Epoch 00031: val_loss improved from 0.12820 to 0.12804, saving model to lstm_300_200_0.30_0.30.h5\n",
      "808580/808580 [==============================] - 278s - loss: 0.2548 - acc: 0.8118 - val_loss: 0.1280 - val_acc: 0.8790\n",
      "Epoch 33/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.8125Epoch 00032: val_loss improved from 0.12804 to 0.12383, saving model to lstm_300_200_0.30_0.30.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808580/808580 [==============================] - 283s - loss: 0.2543 - acc: 0.8125 - val_loss: 0.1238 - val_acc: 0.8840\n",
      "Epoch 34/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.8125Epoch 00033: val_loss did not improve\n",
      "808580/808580 [==============================] - 282s - loss: 0.2541 - acc: 0.8125 - val_loss: 0.1277 - val_acc: 0.8779\n",
      "Epoch 35/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.8128Epoch 00034: val_loss did not improve\n",
      "808580/808580 [==============================] - 285s - loss: 0.2537 - acc: 0.8128 - val_loss: 0.1280 - val_acc: 0.8763\n",
      "Epoch 36/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.8132Epoch 00035: val_loss did not improve\n",
      "808580/808580 [==============================] - 286s - loss: 0.2532 - acc: 0.8132 - val_loss: 0.1282 - val_acc: 0.8760\n",
      "Epoch 37/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.8139Epoch 00036: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2524 - acc: 0.8139 - val_loss: 0.1244 - val_acc: 0.8828\n",
      "Epoch 38/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.8141Epoch 00037: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2521 - acc: 0.8141 - val_loss: 0.1255 - val_acc: 0.8794\n",
      "Epoch 39/1000\n",
      "808448/808580 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.8149Epoch 00038: val_loss did not improve\n",
      "808580/808580 [==============================] - 276s - loss: 0.2518 - acc: 0.8149 - val_loss: 0.1259 - val_acc: 0.8799\n",
      "161716/161716 [==============================] - 20s    \n",
      "161716/161716 [==============================] - 19s    \n",
      "2345472/2345796 [============================>.] - ETA: 0s\n",
      "********************** FOLD  5  ******************\n",
      "\n",
      "lstm_300_200_0.30_0.30\n",
      "Train on 646864 samples, validate on 161716 samples\n",
      "Epoch 1/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.6866Epoch 00000: val_loss improved from inf to 0.23241, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 229s - loss: 0.3918 - acc: 0.6866 - val_loss: 0.2324 - val_acc: 0.7405\n",
      "Epoch 2/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.7284Epoch 00001: val_loss improved from 0.23241 to 0.21224, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.3369 - acc: 0.7284 - val_loss: 0.2122 - val_acc: 0.7650\n",
      "Epoch 3/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.7469Epoch 00002: val_loss improved from 0.21224 to 0.19431, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.3181 - acc: 0.7469 - val_loss: 0.1943 - val_acc: 0.7914\n",
      "Epoch 4/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.7590Epoch 00003: val_loss improved from 0.19431 to 0.19161, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.3061 - acc: 0.7590 - val_loss: 0.1916 - val_acc: 0.7891\n",
      "Epoch 5/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.7678Epoch 00004: val_loss improved from 0.19161 to 0.17973, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2973 - acc: 0.7678 - val_loss: 0.1797 - val_acc: 0.8092\n",
      "Epoch 6/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.7751Epoch 00005: val_loss improved from 0.17973 to 0.17397, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2909 - acc: 0.7751 - val_loss: 0.1740 - val_acc: 0.8192\n",
      "Epoch 7/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.7802Epoch 00006: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2856 - acc: 0.7802 - val_loss: 0.1754 - val_acc: 0.8110\n",
      "Epoch 8/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.7847Epoch 00007: val_loss improved from 0.17397 to 0.16708, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2814 - acc: 0.7847 - val_loss: 0.1671 - val_acc: 0.8263\n",
      "Epoch 9/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.7874Epoch 00008: val_loss improved from 0.16708 to 0.16564, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2780 - acc: 0.7874 - val_loss: 0.1656 - val_acc: 0.8275\n",
      "Epoch 10/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.7903Epoch 00009: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2754 - acc: 0.7903 - val_loss: 0.1662 - val_acc: 0.8257\n",
      "Epoch 11/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.7922Epoch 00010: val_loss improved from 0.16564 to 0.16180, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2735 - acc: 0.7922 - val_loss: 0.1618 - val_acc: 0.8321\n",
      "Epoch 12/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.7951Epoch 00011: val_loss improved from 0.16180 to 0.15645, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2709 - acc: 0.7952 - val_loss: 0.1564 - val_acc: 0.8411\n",
      "Epoch 13/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.7966Epoch 00012: val_loss improved from 0.15645 to 0.15431, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2691 - acc: 0.7966 - val_loss: 0.1543 - val_acc: 0.8449\n",
      "Epoch 14/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.7990Epoch 00013: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2674 - acc: 0.7990 - val_loss: 0.1581 - val_acc: 0.8363\n",
      "Epoch 15/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.8000Epoch 00014: val_loss improved from 0.15431 to 0.15399, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2660 - acc: 0.7999 - val_loss: 0.1540 - val_acc: 0.8428\n",
      "Epoch 16/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.8019Epoch 00015: val_loss improved from 0.15399 to 0.15031, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2644 - acc: 0.8019 - val_loss: 0.1503 - val_acc: 0.8508\n",
      "Epoch 17/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.8026Epoch 00016: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2643 - acc: 0.8026 - val_loss: 0.1553 - val_acc: 0.8402\n",
      "Epoch 18/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.8040Epoch 00017: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2622 - acc: 0.8040 - val_loss: 0.1545 - val_acc: 0.8408\n",
      "Epoch 19/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.8045Epoch 00018: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2619 - acc: 0.8045 - val_loss: 0.1506 - val_acc: 0.8480\n",
      "Epoch 20/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.8064Epoch 00019: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2604 - acc: 0.8064 - val_loss: 0.1510 - val_acc: 0.8464\n",
      "Epoch 21/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.8071Epoch 00020: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2592 - acc: 0.8071 - val_loss: 0.1530 - val_acc: 0.8429\n",
      "Epoch 22/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.8071Epoch 00021: val_loss improved from 0.15031 to 0.14867, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2591 - acc: 0.8071 - val_loss: 0.1487 - val_acc: 0.8516\n",
      "Epoch 23/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.8079Epoch 00022: val_loss improved from 0.14867 to 0.14236, saving model to lstm_300_200_0.30_0.30.h5\n",
      "646864/646864 [==============================] - 227s - loss: 0.2577 - acc: 0.8079 - val_loss: 0.1424 - val_acc: 0.8631\n",
      "Epoch 24/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.8088Epoch 00023: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2571 - acc: 0.8088 - val_loss: 0.1464 - val_acc: 0.8540\n",
      "Epoch 25/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.8103Epoch 00024: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2562 - acc: 0.8103 - val_loss: 0.1470 - val_acc: 0.8530\n",
      "Epoch 26/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.8107Epoch 00025: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2557 - acc: 0.8106 - val_loss: 0.1486 - val_acc: 0.8495\n",
      "Epoch 27/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.8115Epoch 00026: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2547 - acc: 0.8115 - val_loss: 0.1432 - val_acc: 0.8595\n",
      "Epoch 28/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.8123Epoch 00027: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2540 - acc: 0.8123 - val_loss: 0.1475 - val_acc: 0.8521\n",
      "Epoch 29/1000\n",
      "646656/646864 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.8123Epoch 00028: val_loss did not improve\n",
      "646864/646864 [==============================] - 226s - loss: 0.2543 - acc: 0.8123 - val_loss: 0.1441 - val_acc: 0.8571\n",
      "161716/161716 [==============================] - 20s    \n",
      "161716/161716 [==============================] - 19s    \n",
      "2345472/2345796 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "## sample train/validation data\n",
    "\n",
    "data_1_dbl = np.vstack((data_1, data_2))\n",
    "data_2_dbl = np.vstack((data_2, data_1))\n",
    "labels_dbl = np.concatenate((labels, labels))\n",
    "\n",
    "\n",
    "stacking_train = np.empty(shape=(len(data_1_dbl)))\n",
    "stacking_test = np.empty(shape=(len(test_data_1),K))\n",
    "for kth, (train_index, test_index) in enumerate(kf.split(data_1_dbl)):    \n",
    "    print(\"\\n********************** FOLD \", kth+1, \" ******************\\n\")\n",
    "    max_idx_test  = np.amax(test_index)+1\n",
    "    min_idx_test  = np.amin(test_index)\n",
    "    max_idx_train = np.amax(train_index)+1\n",
    "    min_idx_train = np.amin(train_index)\n",
    "    \n",
    "    \n",
    "    weight_val = np.ones(len(labels_dbl[min_idx_test:max_idx_test]))\n",
    "    if re_weight:\n",
    "        weight_val *= 0.472001959\n",
    "        weight_val[labels[min_idx_test:max_idx_test]==0] = 1.309028344\n",
    "        \n",
    "    ########################################\n",
    "    ## define the model structure\n",
    "    model = Sequential()\n",
    "    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "    #lstm_layer0 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm, return_sequences=True)\n",
    "    gru_layer = GRU(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "    #lstm_layer1 = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "    #lstm_layer = Bidirectional(GRU(num_lstm, return_sequences=True))\n",
    "    \n",
    "    \n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = gru_layer(embedded_sequences_1)\n",
    "    #x2 = lstm_layer1(x1)\n",
    "    \n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = gru_layer(embedded_sequences_2)\n",
    "    #y2 = lstm_layer1(y1)\n",
    "    \n",
    "    merged = concatenate([x1, y1])\n",
    "    #Imerged = concatenate([x2, y2])\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    \n",
    "    ########################################\n",
    "    ## add class weight\n",
    "    if re_weight:\n",
    "        class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    else:\n",
    "        class_weight = None\n",
    "    \n",
    "    ########################################\n",
    "    ## train the model\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer='nadam',\n",
    "            metrics=['acc'])\n",
    "    #model.summary()\n",
    "    print(STAMP)\n",
    "    \n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "    bst_model_path = STAMP + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    \n",
    "    batch_size = 512\n",
    "\n",
    "    hist = model.fit([data_1_dbl[min_idx_train:max_idx_train], data_2_dbl[min_idx_train:max_idx_train]], labels_dbl[min_idx_train:max_idx_train], \\\n",
    "        validation_data=([data_1_dbl[min_idx_test:max_idx_test], data_2_dbl[min_idx_test:max_idx_test]], labels_dbl[min_idx_test:max_idx_test], weight_val), \\\n",
    "        epochs=1000, batch_size=batch_size, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "    model.load_weights(bst_model_path)\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "       #predict training ids of the testing fold\n",
    "    \n",
    "    stacking_train[min_idx_test:max_idx_test] = model.predict([data_1_dbl[min_idx_test:max_idx_test], data_2_dbl[min_idx_test:max_idx_test]], batch_size=batch_size, verbose=1)[:,0]\n",
    "    stacking_train[min_idx_test:max_idx_test] += model.predict([data_2_dbl[min_idx_test:max_idx_test], data_1_dbl[min_idx_test:max_idx_test]], batch_size=batch_size, verbose=1)[:,0]\n",
    "    stacking_train[min_idx_test:max_idx_test] /= 2\n",
    "    #pred test\n",
    "    stacking_test[:,kth] = model.predict([test_data_1, test_data_2], batch_size=batch_size, verbose=1)[:,0]\n",
    "    stacking_test[:,kth] += model.predict([test_data_2, test_data_1], batch_size=batch_size, verbose=1)[:,0]\n",
    "    stacking_test[:,kth] /= 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Saving \n",
    "#pred test\n",
    "preds = (stacking_test[:,0] + stacking_test[:,1] + stacking_test[:,2] + stacking_test[:,3] + stacking_test[:,4])/5\n",
    "print(\"Writing output...\")\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = preds\n",
    "sub.to_csv(\"GRU_test_kf\" + preprocessing  + \".csv\", index=False)\n",
    "\n",
    "#--- pred training for ensemble\n",
    "print(\"Writing training pred output...\")\n",
    "sub = pd.DataFrame()\n",
    "sub['is_duplicate'] = stacking_train\n",
    "sub.to_csv(\"GRU_train_kf\" + preprocessing  + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
